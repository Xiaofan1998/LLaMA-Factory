class LlamaSdpaAttention(LlamaAttention):
    # Adapted from LlamaAttention.forward
    def forward(
        self,
        hidden_states: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_value: Optional[Cache] = None,
        output_attentions: bool = False,
        use_cache: bool = False,
        cache_position: Optional[torch.LongTensor] = None,
        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.45
        **kwargs,
    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:
        # 只在 rank 0 的进程上打印参数形状
        if dist.get_rank() == 0:
            print("LlamaSdpaAttention Forward函数参数 (仅在 rank 0 上显示):")
            print(f"hidden_states shape: {hidden_states.shape}")
            print(f"attention_mask shape: {attention_mask.shape if attention_mask is not None else None}")
            print(f"position_ids shape: {position_ids.shape if position_ids is not None else None}")
            print(f"past_key_value: {type(past_key_value)}")
            print(f"output_attentions: {output_attentions}")
            print(f"use_cache: {use_cache}")
            print(f"cache_position shape: {cache_position.shape if cache_position is not None else None}")
            print(f"position_embeddings: {type(position_embeddings)}")

        if output_attentions:
            # ... (保持原有的警告代码不变)

        bsz, q_len, _ = hidden_states.size()

        query_states = self.q_proj(hidden_states)
        key_states = self.k_proj(hidden_states)
        value_states = self.v_proj(hidden_states)

        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)
        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)
        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)

        if position_embeddings is None:
            cos, sin = self.rotary_emb(value_states, position_ids)
        else:
            cos, sin = position_embeddings
        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)

        # ... (保持其余代码不变)

        # 只在 rank 0 的进程上打印中间结果的形状
        if dist.get_rank() == 0:
            print("\nLlamaSdpaAttention 中间结果 (仅在 rank 0 上显示):")
            print(f"query_states shape: {query_states.shape}")
            print(f"key_states shape: {key_states.shape}")
            print(f"value_states shape: {value_states.shape}")
            print(f"causal_mask shape: {causal_mask.shape if causal_mask is not None else None}")
            print(f"is_causal: {is_causal}")
            print(f'is training: {self.training}')

        attn_output = torch.nn.functional.scaled_dot_product_attention(
            query_states,
            key_states,
            value_states,
            attn_mask=causal_mask,
            dropout_p=self.attention_dropout if self.training else 0.0,
            is_causal=is_causal,
        )

        # ... (保持剩余代码不变)

        return attn_output, None, past_key_value